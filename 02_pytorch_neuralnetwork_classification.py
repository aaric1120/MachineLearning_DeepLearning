# -*- coding: utf-8 -*-
"""02_pytorch_neuralnetwork_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dZDAVRx_zegdgfj7xtlK2CI2W-S65ePh

### Nerual Network Classification with Pytorch

Classification is a problem of determining input is something (or multiple)
"""

# 1. Create the data
import sklearn
from sklearn.datasets import make_circles

# Make 1000 Samples
n_samples = 1000

X, y = make_circles(n_samples,
                   noise=0.03,
                   random_state=42)

len(X), len(y)
type(X)

print(X[:5],y[:5])
type(X)

# Make Dataframe of circle data
import pandas as pd
circles = pd.DataFrame({
    "X1" : X[:,0],
    "X2" : X[:, 1],
    "label" : y
})

circles.head(10)
type(X)

# Visualize the data
import matplotlib.pyplot as plt

plt.scatter(x=X[:,0],
            y=X[:,1],
            c=y,
            cmap=plt.cm.RdYlBu
            )
type(X)

"""Note: That the data we're working with is often referred to as a *toy dataset*. Which is a dataset small enough for quick test training, but large enough to matter

### Check input and output shapes
"""

X.shape, y.shape,type(X), type(y)

# View the first example of featrues and labels
X_sample = X[0]
y_sample = y[0]

print(f"Values for one sample of X:{X_sample} and the same for y:{y_sample}")
print(X_sample.shape, y_sample.shape)

"""### Turn data into tensors and create train/test splits


"""

import torch

# change data into tensor

X = torch.from_numpy(X).type(torch.float32)
y = torch.from_numpy(y).type(torch.float32)

X[:5],y[:5]

# Split data into train / test
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2, # make the ratio of train : test = 4 : 1
                                                    random_state=42) # random seed, make the result reproducable

len(X_train),len(X_test),len(y_train),len(y_test)

"""### Building the Model

1. Setting up device agnostic code
2. Construct model with nn.Module
3. Defining a loss function and optimizer
4. Creatign a training loop
"""

import torch
from torch import nn

device = "cuda" if torch.cuda.is_available() else "cpu"
device

len(X_train)

"""NOw that we setup device, let's create a model:

1. Subclasses `nn.Module`
2. Create 2 `nn.Linear()` - Layers that are capable of handling the shapes of our data
3. create a `forward()` method - Forward Propagation
4. Instantiate a and instance
"""

# Construct a model

class CircleModelV1(nn.Module):
  def __init__(self):
    super().__init__()

    # 2. Create 2 nn.linear layers capable of handling the shape of our data
    self.layer_1 = nn.Linear(in_features=2, # input layer, takes in 2 parameters, upscales to 5 output
                             out_features=5) # output of layer, has to match the input of next layer, might be hidden layer, max is 512??
                                            # ------ the upscaled layer that has 5 features IS the HIDDEN LAYER
    self.layer_2 = nn.Linear(in_features=5, # input of second layer, has to match the output of the previous layer
                             out_features=1) # this is the output layer, used to check with labels

  # define a forward method for pass through layers
  def forward(self,x):
    return self.layer_2(self.layer_1(x)) # data goes into layer 1 as input, and output of layer 1 goes into layer 2 as input

# create an instance of the class
model_0 = CircleModelV1().to(device)
model_0

next(model_0.parameters()).device

# Let's replicate the model above using nn.Sequential()

model_00 = nn.Sequential(
    nn.Linear(in_features=2, out_features=5),
    nn.Linear(in_features=5,out_features=1)
).to(device)

model_00

# Make Predictions
model_0.state_dict()

# Make Predictions
with torch.inference_mode(): # always use this when not training
  untrained_preds = model_0(X_test.to(device))
  print(f"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}")

untrained_preds[:10] # the output does not match the values of either 1 or 0 of the labels, this will be an issue

"""### Setup loss function and optimizer

Which optimizer or loss function should you use for classification?

1. For Regression(linear) you might want MAE or MSE
2. For Classification you might want BCE or CCE

As a reminder, the loss function quantifies how much deviation from the label you are.

And for Optimizers, 2 of the most common are SGD and Adam
"""

# Pick a loss function and optimizer

loss_function = nn.BCEWithLogitsLoss() # sigmoid activation function built in. nn.
                                      # BCELoss requires a sigmoid input layer, alse loss stable than WithLogitsLoss()

optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr=0.1)

# Create an Evaluation Metric (Measures how accurate your predictions are, out of 100%)
def accuracy_fn(y_true,y_pred):
  correct = torch.eq(y_true,y_pred).sum().item()
  acc = (correct / len(y_pred)) * 100
  return acc

"""### Training the Model

To Train our Model, we need a training loop:
1. Set hyperparamters, epoch and such
2. set test mode
3. forward propagation
4. loss function calculation
5. Optimizer zero grad
6. Backwards propagation
7. Optimizer Step

### Going from raw logits -> prediction probabilites -> prediction lables

Our Model outputs are going to be raw **logits**.

We can convert these **logits** into prediction probabilities by passing them to some kind of activation function (e.g. sigmoid for binary crossentropy and softmax for multiclass classification)

Then we can convert our model's prediction probabilities to **prediction labels** by either rounding them or taking the argmax();
"""

# View the first 5 outputs of the forward pass on the test data
with torch.inference_mode():
  y_logits = model_0(X_test.to(device)[:5])
y_logits # this won't match with the format of the y labels

# use the sigmoid activation function on our logits
y_pred_probs = torch.sigmoid(y_logits)

"""For our prediction probabilities values, we need to perform a range-style rounding on them:

* `y_pred_probs` >= 0.5, y=1 (class 1)
* `y_pred_probs` < 0.5, y=0 (class 0)
"""

# Find the prediction labels

y_preds = torch.round(y_pred_probs)

# in Full
with torch.inference_mode():
  y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))
  print(y_pred_labels.shape)
  print(y_pred_labels)
  print(y_pred_labels.squeeze())

# Check for equality
print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))

# Get rid of extra dimension
y_preds.squeeze

"""### Building a Training / Testing loop"""

torch.manual_seed(42)
torch.cuda.manual_seed(42)

# set the hyperparameters
epochs = 1000

# Move all necessary tensors to the right device
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

for epoch in range(epochs):
  # 1. Set the training mode
  model_0.train()

  # Forward propagation
  y_logits = model_0(X_train).squeeze()

  # Activating the output of the forwad p output, turns logit -> pred probs -> labels
  y_pred = torch.round(torch.sigmoid(y_logits))

  # calculating the loss and accuracy
  loss = loss_function(y_logits, y_train) # we use logits here cause BCEwithLogits expect raw logits
                                          # If this was the loss function without logit, then the input would have to pass through the sigmoid
                                          # activation function, but not the rounding, so input is torch.sigmoid(Y_logits)

  acc = accuracy_fn(y_true=y_train,
                    y_pred=y_pred)

  # Optimizer zero Grad
  optimizer.zero_grad()

  # Backwards Propagaion
  loss.backward()

  # Update the parameters with Optimizer
  optimizer.step()

  # Testing loop
  # enter evaluation mode
  model_0.eval()

  with torch.inference_mode():
    # 1. forward propagation
    test_logits = model_0(X_test).squeeze() # Figure out why we need a squeeze here...
    test_preds = torch.round(torch.sigmoid(test_logits))

    # Calculate the loss and accuracy
    test_loss = loss_function(test_logits,y_test)
    test_acc = accuracy_fn(y_true=y_test,
                      y_pred=test_preds)

    # Pring out what's happening every 10 epoch
    if epoch % 10 == 0:
      print(f"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")

"""### Make Predictions and evaluate the model

From the metrics, our model isn't learning as planned.

To inspect it, let's make some predictions, and then visualize.

To do so, we're going to import a function called
"""

import requests
from pathlib import Path

# Download helper functions from learn pytorch repo
if Path("helper_functions.py").is_file():
  print("Already exists, doesn't need to download again")
else:
  print("downloading the repo...")
  try:
    request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py")
    with open("helper_functions.py", "wb") as f:
      f.write(request.content)
    print("the module was downloaded successfully...")
  except e:
    print("The download failed...")


from helper_functions import plot_predictions, plot_decision_boundary

# Plot decision boundary of the model
plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.title("Train")
plot_decision_boundary(model_0,X_train,y_train)
plt.subplot(1,2,2)
plt.title("Test")
plot_decision_boundary(model_0,X_test,y_test)

device

y_logits.device

"""### Improving a Model

* Add more layers - give the model more chances to learn the patterns
* Add more hidden units - increase parameters
* Fitting for longer (more epochs)
* Changing the Activation Function
* Change the learning Rate
* Change the loss function
* Use `Transfer Learning`

These options are all from a model's perspective because they are done from altering the model.

The other option is to update the data.

Because all the above suggestions are values we can set, they are known as hyperparameters

## Let's try updating our model by:
1. Adding more hidden units: 5 -> 10
2. Increase the number of layers: 2 -> 3
3. Increase the number of epochs: 100 -> 1000

**PLEASE NOTE NOT TO CHANGE ALL PARAMETERS AT ONCE, CHANGE ONE AND KEEP TRACK IF THE MODEL IMPROVES OR NOT**
"""

# Create an updated model with altered hyperparameters

class CircleModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    # Set the layers and parameters
    self.layer1 = nn.Linear(in_features=2,out_features=10)
    self.layer2 = nn.Linear(in_features=10,out_features=10)
    self.layer3 = nn.Linear(in_features=10, out_features=1)

  # Create the forward function - this outputs the LOGIT!! Have to pass through activation for "pred prob", then rounding for "pred label"
  def forward(self, x):
    return self.layer3(self.layer2(self.layer1(x)))

model_1 = CircleModelV2().to(device)
model_1

# Create the loss function
loss_function = nn.BCEWithLogitsLoss()

# Create the optimizer
optimizer = torch.optim.SGD(model_1.parameters(),
                            lr=0.1)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

# Create the training loop
# 1. set the epochs
epochs = 1000

for epoch in range(epochs):
  # set to testing mode
  model_1.train()

  # do the forward pass
  y_logits = model_1(X_train).squeeze()

  # Create some predictions
  y_pred = torch.round(torch.sigmoid(y_logits))

  # Create the loss function
  loss = loss_function(y_logits,y_train)

  # Create the accuracy function
  acc = accuracy_fn(y_true=y_train,
                    y_pred=y_pred)

  # Zero the gradient
  optimizer.zero_grad()

  # Backwards calculations
  loss.backward()

  # Optimize the parameters
  optimizer.step()

  # Create the testing loop
  model_1.eval()

  with torch.inference_mode():
    # Create the forward pass
    test_logits = model_1(X_test).squeeze()
    # get predictions
    test_pred = torch.round(torch.sigmoid(test_logits))

    # Calculate the loss and accuracy
    test_loss = loss_function(test_logits,y_test)

    acc = accuracy_fn(y_true=y_test,
                      y_pred=test_pred)

    if epoch % 10 == 0:
      print(f"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")

# Plot decision boundaries for training and test sets
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_1, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_1, X_test, y_test)

"""### Creating a linear data set to see if our model is actually learning

"""

# Create some data (same as notebook 01)
weight = 0.7
bias = 0.3
start = 0
end = 1
step = 0.01

# Create data
X_regression = torch.arange(start, end, step).unsqueeze(dim=1)
y_regression = weight * X_regression + bias # linear regression formula

# Check the data
print(len(X_regression))
X_regression[:5], y_regression[:5]

# Create train and test splits
train_split = int(0.8 * len(X_regression)) # 80% of data used for training set
X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]
X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]

# Check the lengths of each split
print(len(X_train_regression),
    len(y_train_regression),
    len(X_test_regression),
    len(y_test_regression))

plot_predictions(train_data=X_train_regression,
    train_labels=y_train_regression,
    test_data=X_test_regression,
    test_labels=y_test_regression
);

# Same architecture as model_1 (but using nn.Sequential)
model_2 = nn.Sequential(
    nn.Linear(in_features=1, out_features=10),
    nn.Linear(in_features=10, out_features=10),
    nn.Linear(in_features=10, out_features=1)
).to(device)

model_2

# Loss and optimizer
loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)

# Train the model
torch.manual_seed(42)

# Set the number of epochs
epochs = 1000

# Put data to target device
X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)
X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)

for epoch in range(epochs):
    ### Training
    # 1. Forward pass
    y_pred = model_2(X_train_regression)

    # 2. Calculate loss (no accuracy since it's a regression problem, not classification)
    loss = loss_fn(y_pred, y_train_regression)

    # 3. Optimizer zero grad
    optimizer.zero_grad()

    # 4. Loss backwards
    loss.backward()

    # 5. Optimizer step
    optimizer.step()

    ### Testing
    model_2.eval()
    with torch.inference_mode():
      # 1. Forward pass
      test_pred = model_2(X_test_regression)
      # 2. Calculate the loss
      test_loss = loss_fn(test_pred, y_test_regression)

    # Print out what's happening
    if epoch % 100 == 0:
        print(f"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}")

# Turn on evaluation mode
model_2.eval()

# Make predictions (inference)
with torch.inference_mode():
    y_preds = model_2(X_test_regression)

# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)
# (try removing .cpu() from one of the below and see what happens)
plot_predictions(train_data=X_train_regression.cpu(),
                 train_labels=y_train_regression.cpu(),
                 test_data=X_test_regression.cpu(),
                 test_labels=y_test_regression.cpu(),
                 predictions=y_preds.cpu());

"""### The Missing Piece - Non Linearity

"What patterns could you draw if you were given an infinite amount of straight and non-straight lines"

Or in machine learning terms, an infinite of linear and non linear lines
"""

# Make and plot data
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles

n_samples = 1000

X, y = make_circles(n_samples=1000,
    noise=0.03,
    random_state=42,
)

plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu);

import torch
from sklearn.model_selection import train_test_split

# turn data in tensors
X = torch.from_numpy(X).type(torch.float32)
y = torch.from_numpy(y).type(torch.float32)

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

X_train[:5],y_train[:5]

# create non linear models
from torch import nn
class CircleModelV3(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer1 = nn.Linear(in_features=2,out_features=10)
    self.layer2 = nn.Linear(in_features=10,out_features=10)
    self.layer3 = nn.Linear(in_features=10,out_features=1)
    # non linear component
    self.relu = nn.ReLU()

  def forward(self,x):
    return self.layer3(self.relu(self.layer2(self.relu(self.layer1(x)))))


# Create an instance
model_3 = CircleModelV3().to(device)
model_3

# setup the loss and optimizer
loss_function = nn.BCEWithLogitsLoss()

optimizer = torch.optim.SGD(model_3.parameters(),
                            lr=0.1)

# Create the training loop and testing
# Set the random seed
torch.manual_seed(42) # -> for CPU
torch.cuda.manual_seed(42) # -> for GPU


# Put all data on target device
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

# set loop count
epochs = 2000

# create the test loop
for epoch in range(epochs):
  # set training mode
  model_3.train()

  # Forward pass
  y_logits = model_3(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))

  # Calculate the loss and accuracy
  loss = loss_function(y_logits, y_train)

  acc = accuracy_fn(y_true=y_train,
                    y_pred=y_pred)


  # zero the optimizer
  optimizer.zero_grad()

  # backwards propagaion
  loss.backward()

  # update the parameters with optimizer
  optimizer.step()

  # Testing loop
  # set testing mode
  model_3.eval()

  with torch.inference_mode():
    # 1. forward pass
    test_logits = model_3(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    # 2. calculate the loss
    test_loss = loss_function(test_logits,y_test)

    # 3. calculate the accuracy
    test_acc = accuracy_fn(y_true=y_test,
                            y_pred=test_pred)

    if epoch % 10 == 0:
      print(f"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%")

# Plot decision boundaries for training and test sets
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity

"""### Replicating non-linear activation functions"""

# Create a small dataset
import torch

A = torch.arange(-10, 10,1,dtype=torch.float32)
A

plt.plot(A)

# Pass through the reLU function
def relu(x):
  return torch.maximum(torch.tensor(0),x)

plt.plot(relu(A))

# pass through the sigmoid function
def sigmoid(x):
  return 1 / (1 + torch.exp(-x))

plt.plot(sigmoid(A))

"""### Creating Multi-Class classification Data

**Binary Classification - output is either yes or no, 1 or 0...binaries**

**Multiclass Classification - output is a probability distribution**

"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split

# Set number of hyper parameters for data creation
NUM_CLASSES = 4
NUM_FEATURES = 2
RANDOM_SEED = 42

# Create multi-class data
x_blob, y_blob = make_blobs(n_samples=1000,
                            n_features=NUM_FEATURES,
                            centers=NUM_CLASSES, # y_labels
                            cluster_std = 1.5, # give the clustes a bit of scatter
                            random_state=RANDOM_SEED)

# turn data into tensors
x_blob = torch.from_numpy(x_blob).type(torch.float32)
y_blob = torch.from_numpy(y_blob).type(torch.float32)

x_blob_train, x_blob_test, y_blob_train, y_blob_test = train_test_split(x_blob,
                                                                        y_blob,
                                                                        test_size=0.2,
                                                                        random_state=RANDOM_SEED)

# PLOT THE DATA
plt.figure(figsize=(10,7))
plt.scatter(x_blob[:,0],x_blob[:,1],c=y_blob,cmap=plt.cm.RdYlBu)

x_blob[:5],y_blob[:5]

"""### Building a multi-class classification model in Pytorch

"""

import torch
from torch import nn

# Create device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

# Build model
class BlobModel(nn.Module):
  def __init__(self,input_features, output_features, hidden_units=8):
    super().__init__()
    self.linear_layer_stack = nn.Sequential(
        nn.Linear(in_features=input_features, out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units,out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units,out_features=output_features)
    )

  # Create the forward function
  def forward(self,x):
    return self.linear_layer_stack(x)

model_4 = BlobModel(input_features=NUM_FEATURES,
                    output_features=NUM_CLASSES,
                    ).to(device)

model_4

# Create loss function and optimizer
loss_function = nn.CrossEntropyLoss() # Best use for multi_class classification

optimizer = torch.optim.SGD(model_4.parameters(),
                            lr=0.1)

# Test the model
model_4.eval()

with torch.inference_mode():
  y_blob_logits = model_4(x_blob_train)

y_blob_logits.shape, y_blob_logits # we need to convert logits -> prediction probability (softmax)

# Convert logits to prediction probabilities
y_pred_probs = torch.softmax(y_blob_logits,dim=1)
y_pred_probs[:4] # these show the probabilities of each index number, the sum of each set should be 1

# Convert prediction probabilities -> Prediction Labels
y_preds = torch.argmax(y_pred_probs, dim=1)
y_preds

"""### Creating a training and testing loop for multi class classification model"""

# Create the training/testing loop
torch.manual_seed(42)
torch.cuda.manual_seed(42)

# Set the training data devices
X_train, y_train = x_blob_train.to(device),y_blob_train.to(device).type(torch.LongTensor)
X_test, y_test = x_blob_test.to(device),y_blob_test.to(device).type(torch.LongTensor)

# Set the epochs
epochs = 1000

# Create the training loop
for epoch in range(epochs):
  # Set the model to training
  model_4.train()

  # Forward pass
  y_logits = model_4(X_train)
  y_pred = torch.argmax(torch.softmax(y_logits,dim=1), dim=1) # logits -> pred prob -> pred labels

  # creat the loss function
  loss = loss_function(y_logits, y_train)

  # create the accuracy function
  acc = accuracy_fn(y_true=y_train,
                    y_pred=y_pred)

  # Zero the gradient for optimizer
  optimizer.zero_grad()

  # backward propagation
  loss.backward()

  # update the parameters with optimizer (gradient descent algorithm)
  optimizer.step()

  ### Testing loop
  model_4.eval()

  with torch.inference_mode():
    # Forward pass and get prediction label
    test_logits = model_4(X_test)
    test_pred = torch.argmax(torch.softmax(test_logits,dim=1),dim=1)

    # Get loss and accuracy
    test_loss = loss_function(test_logits,y_test)

    test_acc = accuracy_fn(y_true=y_test,
                           y_pred=test_pred)

    if epoch % 10 == 0:
      print(f"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%")

# Make predictions
model_4.eval()
with torch.inference_mode():
    y_logits = model_4(X_test)

# View the first 10 predictions
y_logits[:10]

# Turn predicted logits in prediction probabilities
y_pred_probs = torch.softmax(y_logits, dim=1)

# Turn prediction probabilities into prediction labels
y_preds = y_pred_probs.argmax(dim=1)

# Compare first 10 model preds and test labels
print(f"Predictions: {y_preds[:10]}\nLabels: {y_test[:10]}")
print(f"Test accuracy: {accuracy_fn(y_true=y_test, y_pred=y_preds)}%")

# Visualize the data
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_4, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_4, X_test, y_test)